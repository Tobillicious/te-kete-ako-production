#!/usr/bin/env python3
"""
IMMEDIATE GraphRAG Update - Scan All Site Content
Updates Supabase with all 1,071+ pages so agents can coordinate intelligently
"""

import os
import sys
import json
from pathlib import Path
from datetime import datetime
from supabase_graphrag_connector import SupabaseGraphRAGConnector

def scan_all_html_files():
    """Scan all HTML files in public directory"""
    public_dir = Path("public")
    html_files = list(public_dir.rglob("*.html"))
    
    resources = []
    for html_file in html_files:
        try:
            content = html_file.read_text(encoding='utf-8')
            
            # Extract title
            title_match = content.split('<title>')[1].split('</title>')[0] if '<title>' in content else html_file.stem
            
            # Extract description
            desc_match = 'content="' in content and content.split('content="')[1].split('"')[0] if 'description' in content else ""
            
            # Determine type from path
            path_str = str(html_file)
            if 'handouts' in path_str:
                rtype = 'handout'
            elif 'lessons' in path_str:
                rtype = 'lesson'
            elif 'units' in path_str:
                rtype = 'unit'
            elif 'games' in path_str:
                rtype = 'game'
            else:
                rtype = 'page'
            
            # Cultural level
            cultural = 'high' if any(term in content.lower() for term in ['mƒÅori', 'whakataukƒ´', 'tikanga', 'mƒÅtauranga']) else 'medium'
            
            resources.append({
                'title': title_match[:200],
                'path': str(html_file.relative_to(Path("."))),
                'type': rtype,
                'description': desc_match[:500] if desc_match else "",
                'cultural_level': cultural,
                'last_updated': datetime.now().isoformat()
            })
            
        except Exception as e:
            print(f"‚ö†Ô∏è Error processing {html_file}: {e}")
            continue
    
    return resources

def update_graphrag_database(resources):
    """Update Supabase GraphRAG with all resources"""
    connector = SupabaseGraphRAGConnector()
    
    print(f"üìä Updating GraphRAG with {len(resources)} resources...")
    
    # Note: The current connector is read-only
    # We need to add insert capability
    # For now, save to JSON for manual import
    
    with open('graphrag-update-oct14.json', 'w') as f:
        json.dump({
            'session': datetime.now().isoformat(),
            'total_resources': len(resources),
            'resources': resources
        }, f, indent=2)
    
    print(f"‚úÖ Saved to graphrag-update-oct14.json")
    print(f"üìà Resources indexed: {len(resources)}")
    
    # Categorize
    by_type = {}
    for r in resources:
        rtype = r['type']
        by_type[rtype] = by_type.get(rtype, 0) + 1
    
    print("\nüìä Resource Breakdown:")
    for rtype, count in sorted(by_type.items(), key=lambda x: x[1], reverse=True):
        print(f"  {rtype}: {count}")
    
    return True

if __name__ == "__main__":
    print("üß† KAITIAKI ARONUI - GraphRAG Update System")
    print("=" * 60)
    
    print("\n1Ô∏è‚É£ Scanning all HTML files...")
    resources = scan_all_html_files()
    
    print(f"\n2Ô∏è‚É£ Found {len(resources)} resources")
    
    print("\n3Ô∏è‚É£ Updating GraphRAG database...")
    success = update_graphrag_database(resources)
    
    if success:
        print("\n‚úÖ GraphRAG UPDATE COMPLETE!")
        print(f"\nüìã Next steps:")
        print(f"  1. Review: graphrag-update-oct14.json")
        print(f"  2. Import to Supabase (or use connector with insert capability)")
        print(f"  3. Agents can now query {len(resources)} resources!")
    else:
        print("\n‚ùå Update failed")
        sys.exit(1)

