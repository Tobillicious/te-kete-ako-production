#!/usr/bin/env python3
"""
Complete Codebase Scanner - Find EVERY unindexed HTML file
Maps the ENTIRE codebase to GraphRAG with zero gaps
"""
from supabase import create_client
from pathlib import Path
import re
from datetime import datetime

SUPABASE_URL = 'https://nlgldaqtubrlcqddppbq.supabase.co'
SUPABASE_KEY = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6Im5sZ2xkYXF0dWJybGNxZGRwcGJxIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NTMwODkzMzksImV4cCI6MjA2ODY2NTMzOX0.IFaWqep1MBSofARiCUuzvAReC44hwGnmKOMNSd55nIM'

print("üîç COMPLETE CODEBASE SCANNER")
print("=" * 80)
print("Finding EVERY HTML file not yet in GraphRAG...")
print("=" * 80)

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
BASE_DIR = Path(__file__).parent

# Get ALL indexed paths from GraphRAG (with pagination)
print("\nüìä Loading all GraphRAG paths...")
all_indexed = []
offset = 0
page_size = 1000

while True:
    result = supabase.table('graphrag_resources').select('file_path').range(offset, offset + page_size - 1).execute()
    if not result.data:
        break
    all_indexed.extend([r['file_path'] for r in result.data])
    offset += page_size
    if len(result.data) < page_size:
        break

print(f"   Loaded {len(all_indexed)} indexed paths")

# Create comprehensive path matching set
indexed_set = set()
for path in all_indexed:
    # Add exact path
    indexed_set.add(path)
    # Add without leading slash
    indexed_set.add(path.lstrip('/'))
    # Add with leading slash
    indexed_set.add('/' + path.lstrip('/'))
    # Add various combinations
    if path.startswith('public/'):
        indexed_set.add(path[7:])  # Remove 'public/'
        indexed_set.add('/' + path[7:])
    else:
        indexed_set.add('public/' + path.lstrip('/'))
    # Add absolute path
    indexed_set.add(str(BASE_DIR / path))
    indexed_set.add(str(BASE_DIR / path.lstrip('/')))

print(f"   Created {len(indexed_set)} path variations for matching")

# Find ALL HTML files in codebase
print("\nüîç Scanning entire codebase...")
all_html = list(BASE_DIR.rglob('*.html'))
print(f"   Found {len(all_html)} HTML files total")

# Directories to SKIP (not educational content)
skip_patterns = [
    'node_modules',
    '.git',
    '.netlify',
    '__pycache__',
    'dist/node_modules',
    '.cursor',
    'lighthouse',
    '.vscode'
]

# Find unindexed files
unindexed = []
skipped = []

for file_path in all_html:
    rel_path = str(file_path.relative_to(BASE_DIR))
    
    # Skip certain directories
    if any(pattern in rel_path for pattern in skip_patterns):
        skipped.append(rel_path)
        continue
    
    # Check if indexed (any variation)
    is_indexed = False
    for check_path in [rel_path, '/' + rel_path, rel_path.lstrip('/'), str(file_path)]:
        if check_path in indexed_set:
            is_indexed = True
            break
    
    if not is_indexed:
        unindexed.append(file_path)

print(f"   Unindexed: {len(unindexed)} files")
print(f"   Skipped (node_modules etc): {len(skipped)} files")
print(f"   Already indexed: {len(all_html) - len(unindexed) - len(skipped)} files")

# Categorize unindexed
categories = {}
for file_path in unindexed:
    rel_path = str(file_path.relative_to(BASE_DIR))
    
    # Determine category
    if 'backup' in rel_path.lower():
        cat = 'backups'
    elif 'archive' in rel_path.lower():
        cat = 'archives'
    elif '/public/' in rel_path or rel_path.startswith('public/'):
        if '/lessons/' in rel_path:
            cat = 'public/lessons'
        elif '/handouts/' in rel_path:
            cat = 'public/handouts'
        elif '/units/' in rel_path:
            cat = 'public/units'
        elif '/games/' in rel_path:
            cat = 'public/games'
        else:
            cat = 'public/other'
    elif '/lessons/' in rel_path:
        cat = 'lessons'
    elif '/handouts/' in rel_path:
        cat = 'handouts'
    elif '/units/' in rel_path:
        cat = 'units'
    elif 'dist' in rel_path:
        cat = 'dist'
    else:
        cat = 'root/other'
    
    if cat not in categories:
        categories[cat] = []
    categories[cat].append(file_path)

print(f"\nüì¶ Unindexed files by category:")
for cat, files in sorted(categories.items(), key=lambda x: len(x[1]), reverse=True):
    print(f"   {cat}: {len(files)} files")

# Extract metadata function
def extract_metadata(file_path):
    """Extract metadata from HTML file"""
    rel_path = str(file_path.relative_to(BASE_DIR))
    
    try:
        content = file_path.read_text(encoding='utf-8', errors='ignore')
    except Exception as e:
        print(f"   ‚ö†Ô∏è Could not read {file_path.name}: {e}")
        content = ""
    
    # Extract title
    title_match = re.search(r'<title>(.*?)</title>', content, re.IGNORECASE | re.DOTALL)
    if title_match:
        title = title_match.group(1).strip()
        title = re.sub(r'\s+', ' ', title)
        title = re.sub(r'[\n\r\t]', ' ', title)
    else:
        title = file_path.stem.replace('-', ' ').replace('_', ' ').title()
    
    # Determine resource type
    rel_lower = rel_path.lower()
    if '/lessons/' in rel_lower or 'lesson' in rel_lower:
        resource_type = 'Lesson'
    elif '/handouts/' in rel_lower or 'handout' in rel_lower:
        resource_type = 'Handout'
    elif '/units/' in rel_lower or 'unit' in rel_lower:
        resource_type = 'Unit-Plan'
    elif '/games/' in rel_lower or 'game' in rel_lower:
        resource_type = 'Game'
    elif '/assessment' in rel_lower:
        resource_type = 'Assessment'
    elif 'interactive' in rel_lower:
        resource_type = 'Interactive'
    else:
        resource_type = 'Page'
    
    # Subject detection
    subject = 'Multi-subject'
    if any(x in rel_lower for x in ['math', 'algebra', 'geometry', 'statistics']):
        subject = 'Mathematics'
    elif 'science' in rel_lower or 'ecology' in rel_lower or 'biology' in rel_lower:
        subject = 'Science'
    elif 'english' in rel_lower or 'literacy' in rel_lower or 'writing' in rel_lower:
        subject = 'English'
    elif any(x in rel_lower for x in ['social', 'history', 'civics', 'economics']):
        subject = 'Social Studies'
    elif 'digital' in rel_lower or 'technology' in rel_lower or 'tech' in rel_lower:
        subject = 'Digital Technologies'
    elif 'te reo' in rel_lower or 'mƒÅori language' in rel_lower:
        subject = 'Te Reo MƒÅori'
    
    # Quality score based on location
    if 'generated-resources-alpha' in rel_lower:
        quality = 92
    elif 'backup' in rel_lower:
        quality = 86
    elif 'archive' in rel_lower:
        quality = 84
    elif 'dist' in rel_lower:
        quality = 88
    elif '/public/' in rel_path:
        quality = 89
    else:
        quality = 85
    
    # Cultural elements
    content_lower = content.lower()
    has_whakatauaki = bool(re.search(r'whakataukƒ´|whakatauki', content, re.IGNORECASE))
    has_te_reo = bool(re.search(r'te reo|mƒÅori|maori|kia ora|whakapapa|kaitiaki|mƒÅtauranga|iwi', content, re.IGNORECASE))
    cultural_context = has_te_reo or 'cultural' in content_lower or 'indigenous' in content_lower
    
    # Year level
    year_patterns = [
        r'[Yy]ear\s*(\d+)',
        r'Y(\d+)\s',
        r'Level\s*(\d+)',
        r'\bY(\d+)\b'
    ]
    year_level = "All Years"
    for pattern in year_patterns:
        match = re.search(pattern, content + ' ' + rel_path)
        if match:
            num = match.group(1)
            if 7 <= int(num) <= 13:
                year_level = f"Year {num}"
                break
    
    return {
        'file_path': rel_path,
        'resource_type': resource_type,
        'title': title[:200],
        'quality_score': quality,
        'cultural_context': cultural_context,
        'year_level': year_level,
        'subject': subject,
        'has_whakataukƒ´': has_whakatauaki,
        'has_te_reo': has_te_reo,
        'content_preview': content[:500] if content else None,
        'metadata': {
            'source': 'complete_codebase_scan',
            'indexed_date': str(datetime.now().date()),
            'file_size': file_path.stat().st_size,
            'directory': str(file_path.parent.relative_to(BASE_DIR))
        }
    }

# Process in batches
print(f"\nüöÄ Processing {len(unindexed)} unindexed files...")
batch_size = 100
total_indexed = 0
total_errors = 0

for i in range(0, len(unindexed), batch_size):
    batch_files = unindexed[i:i+batch_size]
    batch_num = i // batch_size + 1
    total_batches = (len(unindexed) + batch_size - 1) // batch_size
    
    print(f"\nüì¶ Batch {batch_num}/{total_batches}: Processing {len(batch_files)} files")
    
    batch_records = []
    for file_path in batch_files:
        try:
            record = extract_metadata(file_path)
            batch_records.append(record)
        except Exception as e:
            print(f"   ‚ö†Ô∏è Error processing {file_path.name}: {str(e)[:60]}")
            total_errors += 1
    
    # Insert batch
    if batch_records:
        try:
            supabase.table('graphrag_resources').insert(batch_records).execute()
            total_indexed += len(batch_records)
            print(f"   ‚úÖ Indexed {len(batch_records)} files")
            print(f"   üìä Progress: {total_indexed}/{len(unindexed)} ({total_indexed/len(unindexed)*100:.1f}%)")
        except Exception as e:
            error_msg = str(e)
            if 'duplicate' in error_msg.lower():
                print(f"   ‚ÑπÔ∏è  Some duplicates skipped")
            else:
                print(f"   ‚ùå Error: {error_msg[:100]}")
                total_errors += len(batch_records)
    
    # Checkpoint
    if batch_num % 20 == 0:
        print(f"\n{'='*80}")
        print(f"   CHECKPOINT: {total_indexed} indexed | {total_errors} errors")
        print(f"{'='*80}")

# Final summary
print(f"\n{'='*80}")
print(f"‚úÖ COMPLETE CODEBASE INDEXING FINISHED!")
print(f"{'='*80}")
print(f"   Files processed: {len(unindexed)}")
print(f"   Successfully indexed: {total_indexed}")
print(f"   Errors: {total_errors}")
print(f"   Success rate: {total_indexed/(len(unindexed) or 1)*100:.1f}%")
print(f"{'='*80}")

# Final verification
final_count = supabase.table('graphrag_resources').select('id', count='exact').execute()
print(f"\nüéØ FINAL GRAPHRAG COUNT: {final_count.count} resources")
print(f"   Total HTML files in codebase: {len(all_html)}")
print(f"   Coverage: {final_count.count}/{len(all_html)-len(skipped)} ({final_count.count/(len(all_html)-len(skipped))*100:.1f}%)")
print(f"\nüéâ ENTIRE CODEBASE IS NOW MAPPED TO GRAPHRAG!")

